from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, PredictionErrorDisplay, precision_score, precision_recall_curve
import sklearn.datasets as ds
import matplotlib.pyplot as plt
import statsmodels as sm
import pandas as pd
import seaborn as sns
import numpy as np
from imblearn.over_sampling import RandomOverSampler,SMOTE
import math
import statsmodels.api as sm

import warnings
warnings.filterwarnings('ignore')





df = pd.read_csv("data/CS_data.txt", delimiter="\t")
df.head()


df.info()


df.describe()





df.shape


df.isna().mean() * 100


df.dropna(axis="columns", thresh=0.9 * len(df), inplace=True)


#identifikasi kategorikal dan numerical
categorical2_cols = df.select_dtypes(include=['object', 'category']).columns
numerical2_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col not in categorical2_cols]


categorical2_cols


# Visualisasi bar chart proporsi kategori
for col in categorical2_cols:
    plt.figure(figsize=(6, 3))
    df[col].value_counts(normalize=True).plot(kind='bar', color='skyblue')
    plt.title(f'Proporsi Nilai pada Kolom {col}')
    plt.ylabel('Proporsi')
    plt.xlabel(col)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()



df['RESIDENCIAL_CITY'] = df['RESIDENCIAL_CITY'].str.strip().str.title()


df.RESIDENCIAL_CITY.value_counts()


grouped = df.groupby('RESIDENCIAL_CITY').agg(
    total_pemohon=('TARGET_LABEL_BAD.1', 'count'),
    total_default=('TARGET_LABEL_BAD.1', 'sum')
)
grouped['default_rate'] = grouped['total_default'] / grouped['total_pemohon']



grouped


numerical2_cols


# Atur style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (10, 5)

# 1. Histogram untuk semua kolom numerik
df[numerical_cols].hist(bins=30, figsize=(20, 20), color='skyblue', edgecolor='black')
plt.suptitle("Distribusi Setiap Fitur Numerik", fontsize=20)
plt.tight_layout()
plt.show()


df['TARGET_LABEL_BAD.1'].value_counts()


df.drop(['SEX'],axis=1,inplace=True)


df.columns


target = df[['ID_CLIENT',"AGE","MARITAL_STATUS","QUANT_DEPENDANTS","FLAG_EMAIL","PERSONAL_MONTHLY_INCOME","OTHER_INCOMES","FLAG_VISA","FLAG_MASTERCARD","FLAG_DINERS","FLAG_AMERICAN_EXPRESS","FLAG_OTHER_CARDS","QUANT_CARS","QUANT_BANKING_ACCOUNTS","QUANT_SPECIAL_BANKING_ACCOUNTS","PERSONAL_ASSETS_VALUE","MONTHS_IN_THE_JOB","FLAG_INCOME_PROOF","FLAG_ACSP_RECORD", "RESIDENCE_TYPE", "MONTHS_IN_RESIDENCE", "COMPANY", "FLAG_PROFESSIONAL_PHONE", "TARGET_LABEL_BAD.1"]]
target.head()


target.columns


target.describe()


data_info = pd.DataFrame({"Missing Values ":target.isnull().sum(), "Data Types":target.dtypes})
data_info








target['RESIDENCE_TYPE'] = target['RESIDENCE_TYPE'].astype(str)  # ubah ke string dulu
target['MARITAL_STATUS'] = target['MARITAL_STATUS'].astype(str)  # ubah ke string dulu


target['RESIDENCE_TYPE'].value_counts()


target['MARITAL_STATUS'].value_counts()


#identifikasi kategorikal dan numerical
categorical_cols = target.select_dtypes(include=['object', 'category']).columns
numerical_cols = [col for col in target.select_dtypes(include=['int64', 'float64']).columns if col not in categorical_cols]


# categorical_cols = (
#     target.select_dtypes(include=['object', 'category']).columns.tolist() +
#     ['PROFESSION_CODE']  # kolom yang secara numerik tapi kategorikal
# )
# categorical_cols = list(set(categorical_cols))  # hapus duplikat


numerical_cols


categorical_cols


#define columns transformer to apply to the transformation
preprocessor=ColumnTransformer(
    transformers=[
        ('num',KNNImputer(n_neighbors=5),numerical_cols),
        ('cat',Pipeline(steps=[
            ('convert_to_str',FunctionTransformer(lambda x:x.astype(str),validate=False)),
            ('imputer',SimpleImputer(strategy='constant',fill_value='missing')),
            ('onehot',OneHotEncoder(handle_unknown='ignore'))
        ]),categorical_cols)
    ]
)


#apply transformation
data_preprocessed=preprocessor.fit_transform(target)


#get features name after one hot encoding
ohe_feature_names=preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)


#combine feature names from numeric and cat
all_features_names=numerical_cols + ohe_feature_names.tolist()
all_features_names


#create prreprocessed datagrame
df_2=pd.DataFrame(data_preprocessed,columns=all_features_names)


df_2


#add the target variable back 
# target columns will stick on the original DF
target_variable='TARGET_LABEL_BAD.1'
df_2[target_variable]=df[target_variable]

#calculation the correlation numeric and target
cor=df_2.corr()[target_variable].sort_values(ascending=False)

plt.figure(figsize=(25,20))
sns.heatmap(df_2.corr(),cmap=sns.cubehelix_palette(as_cmap=True),annot=True,fmt='.2f')


cor_df=pd.DataFrame(cor).reset_index()
cor_df.rename(columns={'index':'Feature',target_variable:'Correlation with target'})
cor_df


## Outliers Checking
df_clean=df_2.dropna(subset=['TARGET_LABEL_BAD.1'])


plt.figure(figsize=(20,10))
x=1

# meng assumsi df_clean adalah pre-define dataframe
for column in df_2.describe().columns:
    plt.subplot(10,5,x)
    sns.boxplot(y=column,data=df_clean)
    x+=1
plt.tight_layout()

# cols = df_clean.describe().columns
# n_cols = len(cols)
# n_rows = math.ceil(n_cols / 5)  # 5 kolom per baris

# plt.figure(figsize=(20, n_rows * 3))  # tinggi disesuaikan
# for i, column in enumerate(cols, start=1):
#     plt.subplot(n_rows, 5, i)
#     sns.boxplot(y=column, data=df_clean)
#     plt.title(column)

# plt.tight_layout()
# plt.show()


df_clean_outliers=df_clean.copy()


df_clean_outliers.drop(labels=df_clean_outliers[df_clean_outliers['AGE']==6].index,axis=0,inplace=True)


df_clean_outliers.drop(labels=df_clean_outliers[df_clean_outliers['AGE']==7].index,axis=0,inplace=True)


df_clean_outliers.drop(labels=df_clean_outliers[df_clean_outliers['QUANT_DEPENDANTS']==53].index,axis=0,inplace=True)


df_clean_outliers.drop(['ID_CLIENT', 'FLAG_INCOME_PROOF', 'FLAG_ACSP_RECORD_N'], axis=1, inplace=True)


cols = df_clean_outliers.describe().columns
n_cols = len(cols)
n_rows = math.ceil(n_cols / 5)  # 5 kolom per baris

plt.figure(figsize=(20, n_rows * 3))  # tinggi disesuaikan
for i, column in enumerate(cols, start=1):
    plt.subplot(n_rows, 5, i)
    sns.boxplot(y=column, data=df_clean_outliers)
    plt.title(column)

plt.tight_layout()
plt.show()


df_clean_outliers.columns





df_clean_outliers['QUANT_DEPENDANTS'].value_counts().sort_index()


df_clean_outliers['PERSONAL_MONTHLY_INCOME'].describe()
df_clean_outliers['PERSONAL_MONTHLY_INCOME'].quantile([0.95, 0.99, 0.999])


# Buat kolom baru hasil log transform
df_clean_outliers['PERSONAL_MONTHLY_INCOME'] = np.log1p(df_clean_outliers['PERSONAL_MONTHLY_INCOME'])


df_clean_outliers['OTHER_INCOMES'].min()


df_clean_outliers['OTHER_INCOMES'] = np.log1p(df_clean_outliers['OTHER_INCOMES'])


df_clean_outliers['MONTHS_IN_THE_JOB'].min()


df_clean_outliers['MONTHS_IN_JOB'] = np.log1p(df_clean_outliers['MONTHS_IN_THE_JOB'])


df_clean_outliers.columns


df_clean_outliers['MONTHS_IN_RESIDENCE'].min()


df_clean_outliers['MONTHS_IN_RESIDENCE'] = np.log1p(df_clean_outliers['MONTHS_IN_RESIDENCE'])


df_clean_outliers['PERSONAL_ASSETS_VALUE'].min()


df_clean_outliers['PERSONAL_ASSETS_VALUE'] = np.log1p(df_clean_outliers['PERSONAL_ASSETS_VALUE'])


df_clean_outliers.drop(['RESIDENCE_TYPE_0.0', 'RESIDENCE_TYPE_nan'], axis=1, inplace=True)


df_clean_outliers.columns


df_clean_outliers.isna().mean() * 100


df_clean_outliers.select_dtypes(include=['int64', 'float64']).columns


## process data

#untuk nge drop baris uang ada nan di target variable
df_clean_outliers=df_clean_outliers.dropna(subset=['TARGET_LABEL_BAD.1'])
X=df_clean_outliers.drop('TARGET_LABEL_BAD.1',axis=1)
y=df_clean_outliers['TARGET_LABEL_BAD.1']





X_train_RF, X_test_RF, y_train_RF, y_test_RF = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)


model3 = RandomForestClassifier(n_estimators=100, random_state=42)


model3.fit(X_train_RF, y_train_RF)


model3.score(X_train_RF, y_train_RF), model3.score(X_test_RF, y_test_RF)


#predict
y_pred_RF=model3.predict(X_test_RF)


#overfitting test
# Prediksi data training
y_pred_train_RF = model3.predict(X_train_RF)

# Prediksi data testing
y_pred_test_RF = model3.predict(X_test_RF)

# Akurasi training
acc_RF_base_train = accuracy_score(y_train_RF, y_pred_train_RF)

# Akurasi testing
acc_RF_base = accuracy_score(y_test_RF, y_pred_test_RF)

# Cetak hasil
print(f'training accuracy: {acc_RF_base_train}')
print(f'testing accuracy: {acc_RF_base}')


print(classification_report(y_test_RF, y_pred_test_RF))


# Buat confusion matrix
cm3 = confusion_matrix(y_test_RF, y_pred_test_RF)

# Tampilkan hasilnya
print("Confusion Matrix:")
print(cm3)


print("ROC-AUC Score",roc_auc_score(y_test_RF,y_pred_RF))


#over sampler for the target columns
ROS_RF=RandomOverSampler(random_state=42)
X_ROS,y_ROS=ROS_RF.fit_resample(X,y)


X_train_ROS,X_test_ROS,y_train_ROS,y_test_ROS=train_test_split(X_ROS,y_ROS,test_size=0.3,random_state=42)


ROS_classifier=RandomForestClassifier(n_estimators=100)


ROS_classifier.fit(X_train_ROS,y_train_ROS)


ROS_pred=ROS_classifier.predict(X_test_ROS)


print(confusion_matrix(y_test_ROS,ROS_pred))


print(classification_report(y_test_ROS,ROS_pred))


ROS_classifier.score(X_train_ROS, y_train_ROS), ROS_classifier.score(X_test_ROS, y_test_ROS)


#OVERFIT AFTER ROS
y_pred_train_RF_ROS=ROS_classifier.predict(X_train_ROS)
acc_rf_ROS_base=accuracy_score(y_test_ROS,ROS_pred)


acc_rf_ROS_base_train=accuracy_score(y_train_ROS,y_pred_train_RF_ROS)
print(f"training accuracy {acc_rf_ROS_base_train}")
print(f"testing accuracy {acc_rf_ROS_base}")


print("ROC-AUC Score",roc_auc_score(y_test_ROS,ROS_pred))








df_clean_lr=df_clean.copy()


df_clean_lr.drop(labels=df_clean_lr[df_clean_lr['AGE']==6].index,axis=0,inplace=True)


df_clean_lr.drop(labels=df_clean_lr[df_clean_lr['AGE']==7].index,axis=0,inplace=True)


df_clean_lr.drop(labels=df_clean_lr[df_clean_lr['QUANT_DEPENDANTS']==53].index,axis=0,inplace=True)


df_clean_lr.drop(['ID_CLIENT', 'FLAG_INCOME_PROOF', 'FLAG_ACSP_RECORD_N'], axis=1, inplace=True)


# Cek nilai maksimum
print("Nilai usia tertinggi:", df_clean_lr['AGE'].max())

# Cek berapa banyak yang > 100
outlier_usia = df_clean_lr[df_clean_lr['AGE'] > 100]
print("Jumlah data dengan usia > 100:", len(outlier_usia))
print(outlier_usia)


# Cap (pembatasan nilai maksimum)
df_clean_lr['AGE'] = df_clean_lr['AGE'].apply(lambda x: min(x, 100))


# Visualisasi boxplot
plt.figure(figsize=(8, 4))
sns.boxplot(x=df_clean_lr['AGE'])
plt.title("Distribusi Usia (AGE)")
plt.show()


df_clean_lr['QUANT_DEPENDANTS'].value_counts().sort_index()


df_clean_lr['PERSONAL_MONTHLY_INCOME'].describe()
df_clean_lr['PERSONAL_MONTHLY_INCOME'].quantile([0.95, 0.99, 0.999])


# Buat kolom baru hasil log transform
df_clean_lr['PERSONAL_MONTHLY_INCOME'] = np.log1p(df_clean_lr['PERSONAL_MONTHLY_INCOME'])


df_clean_lr['OTHER_INCOMES'].min()


df_clean_lr['OTHER_INCOMES'] = np.log1p(df_clean_lr['OTHER_INCOMES'])


df_clean_lr['MONTHS_IN_THE_JOB'].min()


df_clean_lr['MONTHS_IN_JOB'] = np.log1p(df_clean_lr['MONTHS_IN_THE_JOB'])


df_clean_lr.columns


df_clean_lr['MONTHS_IN_RESIDENCE'].min()


df_clean_lr['MONTHS_IN_RESIDENCE'] = np.log1p(df_clean_lr['MONTHS_IN_RESIDENCE'])


df_clean_lr['PERSONAL_ASSETS_VALUE'].min()


df_clean_lr['PERSONAL_ASSETS_VALUE'] = np.log1p(df_clean_lr['PERSONAL_ASSETS_VALUE'])


df_clean_lr.drop(['RESIDENCE_TYPE_0.0', 'RESIDENCE_TYPE_nan'], axis=1, inplace=True)


#add the target variable back 
# target columns will stick on the original DF
target_variable='TARGET_LABEL_BAD.1'
df_clean_lr[target_variable]=df[target_variable]

#calculation the correlation numeric and target
cor=df_clean_lr.corr()[target_variable].sort_values(ascending=False)

cor_df=pd.DataFrame(cor).reset_index()
cor_df.rename(columns={'index':'Feature',target_variable:'Correlation with target'})
cor_df





## process data

#untuk nge drop baris uang ada nan di target variable
df_clean_lr=df_clean_lr.dropna(subset=['TARGET_LABEL_BAD.1'])
X=df_clean_lr.drop('TARGET_LABEL_BAD.1',axis=1)
y=df_clean_lr['TARGET_LABEL_BAD.1']


X = df_clean_lr.drop('TARGET_LABEL_BAD.1', axis=1)
y = df_clean_lr['TARGET_LABEL_BAD.1']
X = sm.add_constant(X)
model = sm.Logit(y, X).fit()



categorical_cols = X.select_dtypes(include=['object', 'category']).columns
numerical_cols = [col for col in X.select_dtypes(include=['int64', 'float64']).columns if col not in categorical_cols]


# Pipeline untuk numerik saja
numerical_pipeline = Pipeline(steps=[
    ('scaler', RobustScaler())
])

# ColumnTransformer hanya untuk kolom numerik
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_pipeline, numerical_cols)
], remainder='passthrough')  # kolom non-numerik tetap dipertahankan



#split
X_train_lr, X_test_lr, y_train_lr ,y_test_lr =train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)


model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=100, class_weight='balanced'))
])


model_pipeline.fit(X_train_lr, y_train_lr)


#overfitting test
# Prediksi data training
y_pred_train_lr = model_pipeline.predict(X_train_lr)

# Prediksi data testing
y_pred_test_lr = model_pipeline.predict(X_test_lr)

# Akurasi training
acc_lr_base_train = accuracy_score(y_train_lr, y_pred_train_lr)

# Akurasi testing
acc_lr_base = accuracy_score(y_test_lr, y_pred_test_lr)

# Cetak hasil
print(f'training accuracy: {acc_lr_base_train}')
print(f'testing accuracy: {acc_lr_base}')



print(classification_report(y_test_lr, y_pred_test_lr))


# Buat confusion matrix
cm = confusion_matrix(y_test_lr, y_pred_test_lr)

# Tampilkan hasilnya
print("Confusion Matrix:")
print(cm)


y_pred_prob = model_pipeline.predict(X)
y_pred = (y_pred_prob > 0.5).astype(int)

print(classification_report(y, y_pred))
print("ROC AUC:", roc_auc_score(y, y_pred_prob))


# Threshold dari 0.0 sampai 1.0
thresholds = np.arange(0.0, 1.05, 0.05)
accuracies = []
precisions = []

# Dapatkan probabilitas prediksi (kelas positif)
y_proba = model_pipeline.predict_proba(X_test_lr)[:, 1]

for t in thresholds:
    y_pred_thresh = (y_proba >= t).astype(int)

    acc = accuracy_score(y_test_lr, y_pred_thresh)
    prec = precision_score(y_test_lr, y_pred_thresh, zero_division=0)

    accuracies.append(acc)
    precisions.append(prec)

# Plot Accuracy vs Precision
plt.figure(figsize=(10, 6))
plt.plot(thresholds, accuracies, label='Accuracy', marker='o')
plt.plot(thresholds, precisions, label='Precision', marker='x')
plt.title('Accuracy vs Precision (AvP)')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend()
plt.grid(True)
plt.show()



# Ambil 100 sampel acak
sample_size = 100
idx = np.random.choice(len(y_test_lr), size=sample_size, replace=False)

# Ambil actual dan prediksi berdasarkan posisi
actual = y_test_lr.iloc[idx]
predicted = y_pred_test_lr[idx]

# Visualisasi Actual vs Predicted
plt.figure(figsize=(14, 4))
plt.plot(actual.values, label='Actual', marker='o')
plt.plot(predicted, label='Predicted', marker='x')
plt.title('Actual vs Predicted Labels (Sampled)')
plt.xlabel('Sample Index')
plt.ylabel('Class')
plt.legend()
plt.grid(True)
plt.show()



# Misal df_clean_lr adalah dataframe kamu dan 'TARGET_LABEL_BAD.1' adalah target
X = df_clean_lr.drop(columns=['TARGET_LABEL_BAD.1'])
y = df_clean_lr['TARGET_LABEL_BAD.1']

# Tambahkan konstanta untuk intercept
X_const = sm.add_constant(X)

# Fit model
logit_model = sm.Logit(y, X_const)
result = logit_model.fit()

# Tampilkan ringkasan
print(result.summary())

# Ekstrak summary ke DataFrame
summary_df = result.summary2().tables[1]

# Filter fitur tidak signifikan (p-value > 0.05)
insignificant = summary_df[summary_df['P>|z|'] > 0.05]

# Filter fitur dengan koefisien kecil (misalnya < 0.01)
small_coef = summary_df[np.abs(summary_df['Coef.']) < 0.01]

# Tampilkan
print("\n📌 Fitur dengan p-value > 0.05 (tidak signifikan):")
print(insignificant)

print("\n📌 Fitur dengan koefisien absolut < 0.01:")
print(small_coef)

# Daftar fitur yang bisa dipertimbangkan untuk dihapus (gabungan keduanya, kecuali const)
to_drop = insignificant.index.union(small_coef.index).difference(['const'])
print("\n🗑️ Rekomendasi fitur untuk dipertimbangkan dibuang:")
print(to_drop.tolist())


features_to_drop = [
    'AGE', 'FLAG_AMERICAN_EXPRESS', 'FLAG_DINERS', 'FLAG_EMAIL', 'FLAG_MASTERCARD',
    'FLAG_OTHER_CARDS', 'FLAG_VISA', 'MARITAL_STATUS_0', 'MONTHS_IN_JOB', 
    'MONTHS_IN_RESIDENCE', 'MONTHS_IN_THE_JOB', 'OTHER_INCOMES', 'PERSONAL_ASSETS_VALUE', 
    'PERSONAL_MONTHLY_INCOME', 'QUANT_BANKING_ACCOUNTS', 'QUANT_CARS', 'QUANT_DEPENDANTS', 
    'QUANT_SPECIAL_BANKING_ACCOUNTS', 'RESIDENCE_TYPE_1.0', 'RESIDENCE_TYPE_2.0',
    'RESIDENCE_TYPE_3.0', 'RESIDENCE_TYPE_4.0', 'RESIDENCE_TYPE_5.0'
]



# 1. Siapkan data
df_reduced = df_clean_lr.drop(columns=features_to_drop, errors='ignore')
X = df_reduced.drop(columns=['TARGET_LABEL_BAD.1'])
y = df_reduced['TARGET_LABEL_BAD.1']

# 2. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# 3. Fit Logistic Regression (pakai class_weight='balanced')
model = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')
model.fit(X_train, y_train)

# 4. Evaluasi
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("📊 Classification Report:\n", classification_report(y_test, y_pred))
print("🔁 Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("🚀 ROC AUC Score:", roc_auc_score(y_test, y_prob))


# 1. Siapkan data (dari df_reduced sebelumnya)
X = df_reduced.drop(columns=['TARGET_LABEL_BAD.1'])
y = df_reduced['TARGET_LABEL_BAD.1']

# 2. Train-test split (sebelum SMOTE)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

# 3. Feature scaling (logistic regression sensitif terhadap skala)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Terapkan SMOTE hanya ke data latih
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

print("Jumlah data sebelum SMOTE:", y_train.value_counts().to_dict())
print("Jumlah data sesudah SMOTE:", y_train_res.sum(), "/", len(y_train_res))

# 5. Latih logistic regression
model = LogisticRegression(max_iter=1000, solver='liblinear')
model.fit(X_train_res, y_train_res)

# 6. Evaluasi
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]

print("📊 Classification Report:\n", classification_report(y_test, y_pred))
print("🔁 Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("🚀 ROC AUC Score:", roc_auc_score(y_test, y_prob))


# Dapatkan probabilitas prediksi
y_scores = model.predict_proba(X_test_scaled)[:, 1]

# Ambil precision, recall, threshold
precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)

# Hitung F1 untuk tiap threshold
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
best_index = np.argmax(f1_scores)
best_threshold = thresholds[best_index]

print(f"📌 Best threshold berdasarkan F1-score: {best_threshold:.2f}")
print(f"F1-score pada threshold itu: {f1_scores[best_index]:.4f}")


y_pred_optimal = (y_scores >= 0.45).astype(int)

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

print("📊 Classification Report:\n", classification_report(y_test, y_pred_optimal))
print("🔁 Confusion Matrix:\n", confusion_matrix(y_test, y_pred_optimal))
print("🚀 ROC AUC Score:", roc_auc_score(y_test, y_scores))  # tetap pakai proba

